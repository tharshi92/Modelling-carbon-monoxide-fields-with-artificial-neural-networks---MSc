\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Artificial Neural Networks}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}What is a Neural Network?}{5}}
\newlabel{sec: nn_intro}{{2.1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A single neuron NAND gate with two identical weights, $w_1 = w_2 = -2$ and a bias $b = 3$}}{6}}
\newlabel{fig:simple_neuron}{{2.1}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Architectures, Activation, and Assimilation}{6}}
\newlabel{sec: three_As}{{2.2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Three common activation functions used in neural networks.}}{7}}
\newlabel{fig:act_fun}{{2.2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Universal Approximation Theorem}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The Learning Problem}{8}}
\newlabel{learning_problem}{{2.3}{8}}
\citation{rumelhart1988l}
\newlabel{cost_cost}{{2.4}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Minimizing $E(\Theta )$}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}The Back-propagation Algorithm}{10}}
\newlabel{backprop}{{2.3.2}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}ANNs in Practice}{11}}
\newlabel{ann_practice}{{2.4}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Example: Nonlinear Fitting}{11}}
\newlabel{g}{{2.8}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Example: The Lorentz Model}{11}}
\citation{seidl1991}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Plots of the cost function during the training of the FFNN. Note the minimal over-fitting illustrated by the testing cost curve.}}{12}}
\newlabel{fig:costReg}{{2.3}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The neural network fit of equation 2.8\hbox {}.}}{13}}
\newlabel{fig:nnReg}{{2.4}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The strange attractor solution to the Lorentz system.}}{13}}
\newlabel{fig:3d Lorentz}{{2.5}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The individual components for the strange attractor solution. There is a unique multi-modal structure in which the solution persists between the two modes.}}{14}}
\newlabel{fig:2d Lorentz}{{2.6}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The cost function history for training the Lorentz network. The minimal over-error near the end is relatively negligible.}}{15}}
\newlabel{fig:costs Lorentz}{{2.7}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces The residuals between the trained network and the true dynamical state of the Lorentz system. The network has a good fit until the mode changes, where it loses track of the solution and errors start to accumulate quickly.}}{16}}
\newlabel{fig:nn Lorentz}{{2.8}{16}}
\@setckpt{chapter2}{
\setcounter{page}{17}
\setcounter{equation}{10}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{3}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{0}
\setcounter{parentequation}{0}
}
